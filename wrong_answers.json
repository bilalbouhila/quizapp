{"question_number": 105, "question": "You are designing a data processing pipeline. The pipeline must be able to scale automatically as load increases. Messages must be processed at least once and must be ordered within windows of 1 hour. How should you design the solution?", "user_answer": [], "correct_answer": ["Use Cloud Pub/Sub for message ingestion and Cloud Dataflow for streaming analysis."], "Explanation": "Google Cloud Pub/Sub is a scalable, reliable, fast, and secure real-time messaging service that allows senders to send messages and receivers to receive them. It supports many-to-many, asynchronous messaging that decouples senders and receivers, making it an excellent choice for ingesting data. Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud platform. It provides efficient and automatic resource management and dynamic work rebalancing, which can help with scaling your pipeline as load increases. In this case, Cloud Dataflow's streaming capabilities will support the real-time analysis requirements and ensure that messages are processed at least once.", "All options": ["Use Apache Kafka for message ingestion and use Cloud Dataproc for streaming analysis.", "Use Apache Kafka for message ingestion and use Cloud Dataflow for streaming analysis.", "Use Cloud Pub/Sub for message ingestion and Cloud Dataproc for streaming analysis.", "Use Cloud Pub/Sub for message ingestion and Cloud Dataflow for streaming analysis."]}
{"question_number": 85, "question": "You store historic data in Cloud Storage. You need to perform analytics on the historic data. You want to use a solution to detect invalid data entries and perform data transformations that will not require programming or knowledge of SQL. What should you do?", "user_answer": [], "correct_answer": ["Use Cloud Dataprep with recipes to detect errors and perform transformations."], "Explanation": "Google Cloud Dataprep is an intelligent data service for visually exploring, cleaning, and preparing data for analysis. It is a serverless, no-code data preparation tool that is used to clean and transform data for analysis without the need for programming or SQL knowledge. It uses recipes, which are sets of steps to transform your data.", "All options": ["Use Cloud Dataflow with Beam to detect errors and perform transformations.", "Use Cloud Dataprep with recipes to detect errors and perform transformations.", "Use Cloud Dataproc with a Hadoop job to detect errors and perform transformations.", "Use federated tables in BigQuery with queries to detect errors and perform transformations."]}
